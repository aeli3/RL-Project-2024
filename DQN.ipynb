{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7466442,"sourceType":"datasetVersion","datasetId":4346131},{"sourceId":7466448,"sourceType":"datasetVersion","datasetId":4346133}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import gymnasium as gym\nfrom gymnasium import spaces\nimport numpy as np\nimport time\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nfrom mpl_toolkits import mplot3d\nimport time\nimport random\nimport pandas as pd\nimport time\nimport torch \nimport torch.nn as nn\nimport torch.optim as optim\n\nimport torch.nn.functional as F\nfrom collections import deque\nfrom mpl_toolkits import mplot3d\n\n#Set the seed for reproducibility\nnp.random.seed(10)\n\ndef add_features(df):\n    df['month'] = df['PRICES'].dt.month - 1\n    df['day'] = df['PRICES'].dt.day - 1\n    df['year'] = df['PRICES'].dt.year\n    df['day_of_week'] = df['PRICES'].dt.dayofweek\n    df['day_of_week'] = df['day_of_week'].astype(int)\n    df = df.drop(columns=[\"PRICES\"])\n    return df\n\ntrain = pd.read_excel(\"train.xlsx\", parse_dates=['PRICES'])\nval = pd.read_excel(\"validate.xlsx\", parse_dates=['PRICES'])\n\ntrain = add_features(train)\nval = add_features(val)\n\nprint(train.head(3))  ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-26T13:00:30.866767Z","iopub.execute_input":"2024-01-26T13:00:30.867271Z","iopub.status.idle":"2024-01-26T13:00:32.594515Z","shell.execute_reply.started":"2024-01-26T13:00:30.867228Z","shell.execute_reply":"2024-01-26T13:00:32.593145Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"/kaggle/input/smartgridtrain/train.xlsx\n/kaggle/input/smartgridvalidate/validate.xlsx\n   Hour 01  Hour 02  Hour 03  Hour 04  Hour 05  Hour 06  Hour 07  Hour 08  \\\n0    24.31    24.31    21.71     8.42     0.01     0.01     0.02     0.01   \n1    16.01    11.00     9.01     7.50     9.00     7.45    16.50    28.01   \n2    28.00    24.50    24.15    18.99    15.50    23.11    29.01    39.73   \n\n   Hour 09  Hour 10  ...  Hour 19  Hour 20  Hour 21  Hour 22  Hour 23  \\\n0     0.01     6.31  ...    37.99    33.11    37.99    33.00    36.48   \n1    29.96    39.60  ...    59.69    50.09    50.00    36.22    31.09   \n2    43.81    49.09  ...    60.99    55.51    51.77    34.51    39.31   \n\n   Hour 24  month  day  year  day_of_week  \n0    30.65      0    0  2007            0  \n1    29.84      0    1  2007            1  \n2    38.05      0    2  2007            2  \n\n[3 rows x 28 columns]\n","output_type":"stream"}]},{"cell_type":"code","source":"class SmartGridEnv(gym.Env):\n    def __init__(self, price_data, battery_capacity=50, max_power=25, efficiency=0.9):\n        super(SmartGridEnv, self).__init__()\n        np.random.seed(10)\n        \n        self.price_data = price_data\n        self.battery_capacity = battery_capacity\n        self.max_power = max_power\n        self.efficiency = efficiency\n        self.action_space = gym.spaces.Discrete(51, start=-25) \n        self.time_constraint = 6\n        self.battery_time_constraint = 20\n        self.initialize_params()\n           \n        \n\n    def initialize_params(self):\n        self.current_step = 0\n        self.current_month = 0\n        self.current_day = 0\n        self.current_hour = 0\n        self.day_of_week = int(self.price_data.iloc[self.current_step]['day_of_week'])\n        self.current_battery = 20\n        self.available = True\n        self.current_price = self.price_data.iloc[self.current_step, self.current_hour]\n        self.past_prices = np.zeros(24)\n        self.done = False\n        self.profit = 0 \n        self.current_battery_norm = self.current_battery/50\n        self.current_hour_norm = self.current_hour/23\n        self.past_prices_max = 2500\n        self.past_prices_norm = self.past_prices/self.past_prices_max\n        self.current_state = tuple([self.current_hour, self.current_battery, self.current_price])\n        self.no_features = len(self.current_state)\n         \n        \n    def car_available(self):\n        if self.current_hour == 7:\n            available = np.random.uniform(0, 1) < 0.5\n        else:\n            available = True\n        return available\n    \n\n    def update_state(self):\n        # If the car is not available, it returns at 6pm with 20kW less\n        if self.available == False:\n            self.current_hour = 17\n            self.current_battery -= 20\n        else:\n            self.current_hour = int((self.current_hour + 1) % 24)\n            if self.current_hour == 0:\n                self.current_step += 1\n                \n        self.current_month = int(self.price_data.iloc[self.current_step]['month'])\n        self.current_day = int(self.price_data.iloc[self.current_step]['day'])\n        self.day_of_week = int(self.price_data.iloc[self.current_step]['day_of_week'])\n        self.current_price = self.price_data.iloc[self.current_step, self.current_hour]\n        \n        # Update past day prices\n        if self.current_step > 0:\n            self.past_prices = np.concatenate([np.array(self.price_data.iloc[self.current_step-1, self.current_hour:24]),np.array(train.iloc[self.current_step,0:self.current_hour])])            \n        if (self.current_step+1) % 100 == 0:\n            local_max = np.max(self.price_data.iloc[self.current_step-99:self.current_step, 0:24].values)\n            if local_max > self.past_prices_max:\n                self.past_prices_max = local_max\n            \n        \n        # Update state\n        self.current_state = tuple([self.current_hour, self.current_battery,self.current_price])\n\n        # Check if the episode is done (after 3 years of past electricity prices)\n        if self.current_step >= len(self.price_data) - 1:\n            self.done = True\n\n    def to_discrete(self, action):\n        mask = np.zeros(self.action_space.n, dtype=np.int8)\n        mask[action] = 1\n        return self.action_space.sample(mask = mask)\n    \n    def reward(self, action, charge_cost):\n        reward = -charge_cost \n        return reward\n        \n    def step(self, action):\n\n        self.available=self.car_available()\n        \n        # discretize action\n        if self.available == True:\n            action = self.to_discrete(action) \n        else:\n             action = 0\n        \n        # charge cost\n        energy_rate = self.price_data.iloc[self.current_step, self.current_hour]\n        charge_cost = action if action < 0 else action* 2 \n        charge_cost *= energy_rate / 1000\n        \n        # update battery\n        actual_charge = np.round(self.efficiency * action).astype(int) if action>0 else np.round(action/self.efficiency).astype(int)\n        self.current_battery += actual_charge\n        \n        # update variables\n        self.profit += -charge_cost\n\n        # reward\n        reward = self.reward(action, charge_cost)\n        self.update_state()\n        \n        return self.current_state, reward, self.done, self.available\n    \n\n    def mask(self):\n        # 7 am constraint  \n        if self.current_hour == self.time_constraint and self.current_battery >= self.battery_time_constraint:\n            lower_bound = min(np.ceil((self.battery_time_constraint- self.current_battery)*0.9), self.max_power)\n            upper_bound =  min(np.floor((self.battery_capacity-self.current_battery)/0.9), self.max_power)\n            mask_range = (lower_bound, upper_bound)\n        elif self.current_hour == self.time_constraint and self.current_battery < self.battery_time_constraint:\n            lower_bound = min(np.ceil((self.battery_time_constraint-self.current_battery)/0.9), self.max_power)\n            upper_bound =  min(np.floor((self.battery_capacity-self.current_battery)/0.9), self.max_power)\n            mask_range = (lower_bound, upper_bound)\n        else: \n            lower_bound = max(np.ceil(-(self.current_battery)*0.9), -self.max_power)\n            upper_bound = min(np.floor((self.battery_capacity - self.current_battery)/0.9), self.max_power)\n            mask_range = (lower_bound, upper_bound)\n            \n        \n        # construct boolean mask_vector\n        mask = np.arange(self.action_space.start, self.action_space.start + self.action_space.n)\n        mask = np.where((mask >= mask_range[0]) & (mask <= mask_range[1]), True, False)\n        return mask\n    \n    def reset(self):\n        self.initialize_params()\n        return self.current_state\n    \n    def normalize(self):\n        self.current_battery_norm = self.current_battery/51\n        self.current_hour_norm = self.current_hour/23\n        self.past_prices_norm = self.past_price/self.past_prices_max\n        return self.current_battery_norm, self.current_hour_norm, self.past_prices_norm\n        \ntrain_env=SmartGridEnv(train)","metadata":{"execution":{"iopub.status.busy":"2024-01-26T13:00:32.596887Z","iopub.execute_input":"2024-01-26T13:00:32.597598Z","iopub.status.idle":"2024-01-26T13:00:32.633947Z","shell.execute_reply.started":"2024-01-26T13:00:32.597561Z","shell.execute_reply":"2024-01-26T13:00:32.632487Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"class DQN(nn.Module):\n\n    def __init__(self, env, learning_rate):\n        \n        '''\n        Params:\n        env = environment that the agent needs to play\n        learning_rate = learning rate used in the update\n        \n        '''\n        super(DQN,self).__init__()\n        input_features =  len(env.current_state)\n        action_space = env.action_space.n\n        \n    \n        self.dense1 = nn.Linear(in_features = input_features, out_features = 64)\n        self.dense2 = nn.Linear(in_features = 64, out_features = 64)\n        self.dense3 = nn.Linear(in_features = 64, out_features = 64)\n        self.dense4 = nn.Linear(in_features = 64, out_features = action_space)\n        \n        #Here we use ADAM, but you could also think of other algorithms such as RMSprob\n        self.optimizer = optim.Adam(self.parameters(), lr = learning_rate)\n        \n    def forward(self, x):\n        \n        '''\n        Params:\n        x = observation\n        '''        \n        x = torch.tanh(self.dense1(x))\n        x = torch.tanh(self.dense2(x))\n        x = torch.tanh(self.dense3(x))\n        x = self.dense4(x)\n        \n        return x\n    ","metadata":{"execution":{"iopub.status.busy":"2024-01-26T13:00:32.635476Z","iopub.execute_input":"2024-01-26T13:00:32.635946Z","iopub.status.idle":"2024-01-26T13:00:32.654430Z","shell.execute_reply.started":"2024-01-26T13:00:32.635904Z","shell.execute_reply":"2024-01-26T13:00:32.653000Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"class ExperienceReplay:\n    \n    def __init__(self, env, buffer_size, min_replay_size = 1000 ):\n        \n        '''\n        Params:\n        env = environment that the agent needs to play\n        buffer_size = max number of transitions that the experience replay buffer can store\n        min_replay_size = min number of (random) transitions that the replay buffer needs to have when initialized\n        seed = seed for random number generator for reproducibility\n        '''\n        self.env = env\n        self.min_replay_size = min_replay_size\n        self.replay_buffer = deque(maxlen=buffer_size)\n        self.reward_buffer = deque([0.0], maxlen = 100)\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        \n        print('Please wait, the experience replay buffer will be filled with random transitions')\n                \n        state = self.env.reset() \n        for _ in range(self.min_replay_size):\n            \n            mask = self.env.mask()\n            action = np.random.choice(np.arange(env.action_space.n)[mask])\n            new_state, reward, done, available = env.step(action) #step takes in an action from -25 to 25\n            \n            new_state_mask = self.env.mask()\n            transition = (state, action, reward, done, new_state, new_state_mask)\n            if len(transition)<6:\n                print('errror in initialization!', transition)\n            \n            self.replay_buffer.append(transition)\n            state = new_state\n\n            if done:\n                state= env.reset()\n        \n        print('Initialization with random transitions is done!')\n      \n          \n    def add_data(self, data): \n        '''\n        Params:\n        data = relevant data of a transition, i.e. action, new_obs, reward, done\n        '''\n        self.replay_buffer.append(data)\n            \n    def sample(self, batch_size):\n        \n        '''\n        Params:\n        batch_size = number of transitions that will be sampled\n        \n        Returns:\n        tensor of observations, actions, rewards, done (boolean) and next observation \n        '''\n        \n        transitions = random.sample(self.replay_buffer, batch_size)\n        for t in transitions:\n            if len(t)<6:\n                print('error!', t)\n        observations = np.asarray([t[0] for t in transitions])\n        actions = np.asarray([t[1] for t in transitions])\n        rewards = np.asarray([t[2] for t in transitions])\n        dones = np.asarray([t[3] for t in transitions])\n        new_observations = np.asarray([t[4] for t in transitions])        \n        new_masks = np.asarray([t[5] for t in transitions])\n    \n        #PyTorch needs these arrays as tensors!, don't forget to specify the device! (cpu / GPU)\n        observations_t = torch.as_tensor(observations, dtype = torch.float32, device=self.device)\n        actions_t = torch.as_tensor(actions, dtype = torch.int64, device=self.device).unsqueeze(-1)\n        rewards_t = torch.as_tensor(rewards, dtype = torch.float32, device=self.device).unsqueeze(-1)\n        dones_t = torch.as_tensor(dones, dtype = torch.float32, device=self.device).unsqueeze(-1)\n        new_observations_t = torch.as_tensor(new_observations, dtype = torch.float32, device=self.device)\n        new_masks_t = torch.as_tensor(new_masks, dtype = torch.bool, device=self.device)\n\n        \n        return observations_t, actions_t, rewards_t, dones_t, new_observations_t, new_masks_t\n    \n    def add_reward(self, reward):\n        \n        '''\n        Params:\n        reward = reward that the agent earned during an episode of a game\n        '''\n        self.reward_buffer.append(reward)\n        ","metadata":{"execution":{"iopub.status.busy":"2024-01-26T13:00:32.658424Z","iopub.execute_input":"2024-01-26T13:00:32.658929Z","iopub.status.idle":"2024-01-26T13:00:32.681002Z","shell.execute_reply.started":"2024-01-26T13:00:32.658885Z","shell.execute_reply":"2024-01-26T13:00:32.679705Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"class vanilla_DQNAgent:\n    \n    def __init__(self, env, device, epsilon_decay, \n                 epsilon_start, epsilon_end, discount_rate, lr, buffer_size, seed = 123):\n        '''\n        Params:\n        env = environment that the agent needs to play\n        device = set up to run CUDA operations\n        epsilon_decay = Decay period until epsilon start -> epsilon end\n        epsilon_start = starting value for the epsilon value\n        epsilon_end = ending value for the epsilon value\n        discount_rate = discount rate for future rewards\n        lr = learning rate\n        buffer_size = max number of transitions that the experience replay buffer can store\n        seed = seed for random number generator for reproducibility\n        '''\n        self.env = env\n        self.device = device\n        self.epsilon_decay = epsilon_decay\n        self.epsilon_start = epsilon_start\n        self.epsilon_end = epsilon_end\n        self.discount_rate = discount_rate\n        self.learning_rate = lr\n        self.buffer_size = buffer_size\n        \n        self.replay_memory = ExperienceReplay(self.env, self.buffer_size)\n        self.online_network = DQN(self.env, self.learning_rate).to(self.device)\n        \n    def choose_action(self, step, observation, greedy = False):\n        \n        '''\n        Params:\n        step = the specific step number \n        observation = observation input\n        greedy = boolean that\n        \n        Returns:\n        action: action chosen (either random or greedy)\n        epsilon: the epsilon value that was used \n        '''\n        \n        epsilon = np.interp(step, [0, self.epsilon_decay], [self.epsilon_start, self.epsilon_end])\n        random_sample = random.random()\n        mask = self.env.mask()\n        \n        if (random_sample <= epsilon) and not greedy:\n            #Random action\n            action = np.random.choice(np.arange(self.env.action_space.n)[mask])\n        else:\n            #Greedy action\n            obs_t = torch.as_tensor(observation, dtype = torch.float32, device=self.device)\n            q_values = self.online_network(obs_t.unsqueeze(0))\n            max_q_index = torch.as_tensor(torch.argmax(q_values.squeeze(0)[mask]).item() + np.where(mask)[0][0])\n            action = max_q_index.detach().item()  \n        return action, epsilon\n    \n    def learn(self, batch_size):\n        \n        '''\n        Here we do gradient descent      \n        Params:\n        batch_size = number of transitions that will be sampled\n        '''\n        \n        \n        #Sample random transitions with size = batch size\n        observations_t, actions_t, rewards_t, dones_t, new_observations_t, new_masks_t = self.replay_memory.sample(batch_size)\n        target_q_values = self.online_network(new_observations_t)\n        target_q_values[~new_masks_t] = -50\n        max_target_q_values = target_q_values.max(dim=1, keepdim=True)[0] \n        targets = rewards_t + self.discount_rate * (1-dones_t) * max_target_q_values\n        \n\n        #Compute loss\n        q_values = self.online_network(observations_t)\n        \n        action_q_values = torch.gather(input=q_values, dim=1, index=actions_t)\n        #Loss:  Huber loss\n        loss = F.smooth_l1_loss(action_q_values, targets.detach())\n        #Uncomment this line to use the standard MSE loss\n        #loss = F.mse_loss(action_q_values, targets.detach())\n\n        #Solution:\n        #Gradient descent\n        self.online_network.optimizer.zero_grad()\n        loss.backward()\n        self.online_network.optimizer.step()\n        \n    def predict(self):\n        '''\n        Params:\n        step = the number of the step within the epsilon decay that is used for the epsilon value of epsilon-greedy\n        seed = seed for random number generator for reproducibility\n        '''\n        \n        action = self.choose_action(self.env.current_step, self.env.current_state, True)[0] \n        \n        return action\n    \n        ","metadata":{"execution":{"iopub.status.busy":"2024-01-26T13:00:32.683058Z","iopub.execute_input":"2024-01-26T13:00:32.683549Z","iopub.status.idle":"2024-01-26T13:00:32.705195Z","shell.execute_reply.started":"2024-01-26T13:00:32.683504Z","shell.execute_reply":"2024-01-26T13:00:32.704142Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"#Set the hyperparameters\n\n#Discount rate\ndiscount_rate = 0.9\n#That is the sample that we consider to update our algorithm\nbatch_size = 32\n#Maximum number of transitions that we store in the buffer\nbuffer_size = 5000\n#Minimum number of random transitions stored in the replay buffer\nmin_replay_size = 1000\n#Starting value of epsilon\nepsilon_start = 1.0\n#End value (lowest value) of epsilon\nepsilon_end = 0.1\n#Decay period until epsilon start -> epsilon end\nepsilon_decay = 10000\n\nmax_episodes = 250000\n\n#Learning_rate\nlr = 7e-4\n\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nvanilla_agent = vanilla_DQNAgent(train_env, device, epsilon_decay, epsilon_start, epsilon_end, discount_rate, lr, buffer_size)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-26T13:00:32.706827Z","iopub.execute_input":"2024-01-26T13:00:32.707518Z","iopub.status.idle":"2024-01-26T13:00:33.855685Z","shell.execute_reply.started":"2024-01-26T13:00:32.707462Z","shell.execute_reply":"2024-01-26T13:00:33.854388Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"Please wait, the experience replay buffer will be filled with random transitions\nInitialization with random transitions is done!\n","output_type":"stream"}]},{"cell_type":"code","source":"def training_loop(env, agent, max_episodes, target_ = False, seed=42):\n    \n    '''\n    Params:\n    env = the environment that the agent needs to play\n    agent= which agent is used to train\n    max_episodes = maximum number of games played\n    target = boolean variable indicating if a target network is used (this will be clear later)\n    seed = seed for random number generator for reproducibility\n    \n    Returns:\n    average_reward_list = a list of averaged rewards over 100 episodes of playing the game\n    '''\n\n    env.action_space.seed(seed)\n    obs=env.reset()\n    average_reward_list = [0.0]\n    episode_reward = 0.0\n    \n    for step in range(max_episodes):\n        \n        action, epsilon = agent.choose_action(step, obs)\n       \n        new_state, reward, done, available  = env.step(action)\n        new_mask = env.mask()\n        new_obs = new_state\n        \n        transition = (obs, action, reward, done, new_obs, new_mask)\n        agent.replay_memory.add_data(transition)\n        obs = new_obs\n    \n        episode_reward += reward\n    \n        if done:\n            obs= env.reset()\n            agent.replay_memory.add_reward(episode_reward)\n            #Reinitilize the reward to 0.0 after the game is over\n            episode_reward = 0.0\n\n        #Learn\n        agent.learn(batch_size)\n\n        #Calculate after each 100 episodes an average that will be added to the list\n                \n        if (step+1) % 100 == 0:\n            average_reward_list.append(np.mean(agent.replay_memory.reward_buffer))\n        \n        #Update target network, do not bother about it now!\n        if target_:\n            \n            #Set the target_update_frequency\n            target_update_frequency = 250\n            if step % target_update_frequency == 0:\n                dagent.update_target_network()\n    \n        #Print some output\n        if (step+1) % 10000 == 0:\n            print(20*'--')\n            print('Step', step)\n            print('Epsilon', epsilon)\n            print('Avg Rew', np.mean(agent.replay_memory.reward_buffer))\n            print()\n            \n    return average_reward_list\n\naverage_rewards_vanilla_dqn = training_loop(train_env, vanilla_agent, max_episodes)","metadata":{"execution":{"iopub.status.busy":"2024-01-26T13:00:33.857838Z","iopub.execute_input":"2024-01-26T13:00:33.858196Z","iopub.status.idle":"2024-01-26T13:17:13.556521Z","shell.execute_reply.started":"2024-01-26T13:00:33.858166Z","shell.execute_reply":"2024-01-26T13:17:13.555352Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"----------------------------------------\nStep 9999\nEpsilon 0.1000899999999999\nAvg Rew 0.0\n\n----------------------------------------\nStep 19999\nEpsilon 0.1\nAvg Rew 0.0\n\n----------------------------------------\nStep 29999\nEpsilon 0.1\nAvg Rew -1751.3271300000013\n\n----------------------------------------\nStep 39999\nEpsilon 0.1\nAvg Rew -1751.3271300000013\n\n----------------------------------------\nStep 49999\nEpsilon 0.1\nAvg Rew -1882.1347666666663\n\n----------------------------------------\nStep 59999\nEpsilon 0.1\nAvg Rew -1882.1347666666663\n\n----------------------------------------\nStep 69999\nEpsilon 0.1\nAvg Rew -1962.2977349999992\n\n----------------------------------------\nStep 79999\nEpsilon 0.1\nAvg Rew -1962.2977349999992\n\n----------------------------------------\nStep 89999\nEpsilon 0.1\nAvg Rew -2005.888204\n\n----------------------------------------\nStep 99999\nEpsilon 0.1\nAvg Rew -2005.888204\n\n----------------------------------------\nStep 109999\nEpsilon 0.1\nAvg Rew -2012.9473899999994\n\n----------------------------------------\nStep 119999\nEpsilon 0.1\nAvg Rew -2012.9473899999994\n\n----------------------------------------\nStep 129999\nEpsilon 0.1\nAvg Rew -2013.9839299999992\n\n----------------------------------------\nStep 139999\nEpsilon 0.1\nAvg Rew -2013.9839299999992\n\n----------------------------------------\nStep 149999\nEpsilon 0.1\nAvg Rew -2017.315097499999\n\n----------------------------------------\nStep 159999\nEpsilon 0.1\nAvg Rew -2017.315097499999\n\n----------------------------------------\nStep 169999\nEpsilon 0.1\nAvg Rew -2013.7330166666643\n\n----------------------------------------\nStep 179999\nEpsilon 0.1\nAvg Rew -2013.7330166666643\n\n----------------------------------------\nStep 189999\nEpsilon 0.1\nAvg Rew -2013.7330166666643\n\n----------------------------------------\nStep 199999\nEpsilon 0.1\nAvg Rew -2018.9214259999976\n\n----------------------------------------\nStep 209999\nEpsilon 0.1\nAvg Rew -2018.9214259999976\n\n----------------------------------------\nStep 219999\nEpsilon 0.1\nAvg Rew -2031.8288872727235\n\n----------------------------------------\nStep 229999\nEpsilon 0.1\nAvg Rew -2031.8288872727235\n\n----------------------------------------\nStep 239999\nEpsilon 0.1\nAvg Rew -2039.7521208333294\n\n----------------------------------------\nStep 249999\nEpsilon 0.1\nAvg Rew -2039.7521208333294\n\n","output_type":"stream"}]},{"cell_type":"code","source":"eval_env = SmartGridEnv(val)\n\nvanilla_agent.env = eval_env\ni=0\nwhile not vanilla_agent.env.done:\n    current_state = tuple(vanilla_agent.env.current_state)\n    action = vanilla_agent.predict()\n    if i<100:\n        print(f\"at time {vanilla_agent.env.current_hour +  1} agent transacts {vanilla_agent.env.to_discrete(action)} KWh, battery is {vanilla_agent.env.current_battery}\")\n    next_state, reward, done, available = vanilla_agent.env.step(action)\n    i=i+1\n\nprint(\"Profit on validation set: \", eval_env.profit)","metadata":{"execution":{"iopub.status.busy":"2024-01-26T13:17:13.558152Z","iopub.execute_input":"2024-01-26T13:17:13.558583Z","iopub.status.idle":"2024-01-26T13:17:35.016035Z","shell.execute_reply.started":"2024-01-26T13:17:13.558542Z","shell.execute_reply":"2024-01-26T13:17:35.014706Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stdout","text":"at time 1 agent transacts -18 KWh, battery is 20\nat time 2 agent transacts 8 KWh, battery is 0\nat time 3 agent transacts 19 KWh, battery is 7\nat time 4 agent transacts 8 KWh, battery is 24\nat time 5 agent transacts 8 KWh, battery is 31\nat time 6 agent transacts 8 KWh, battery is 38\nat time 7 agent transacts 1 KWh, battery is 45\nat time 8 agent transacts 1 KWh, battery is 46\nat time 18 agent transacts 19 KWh, battery is 26\nat time 19 agent transacts 1 KWh, battery is 43\nat time 20 agent transacts 1 KWh, battery is 44\nat time 21 agent transacts 1 KWh, battery is 45\nat time 22 agent transacts 1 KWh, battery is 46\nat time 23 agent transacts 1 KWh, battery is 47\nat time 24 agent transacts 1 KWh, battery is 48\nat time 1 agent transacts -18 KWh, battery is 49\nat time 2 agent transacts -18 KWh, battery is 29\nat time 3 agent transacts -1 KWh, battery is 9\nat time 4 agent transacts -1 KWh, battery is 8\nat time 5 agent transacts 19 KWh, battery is 7\nat time 6 agent transacts 8 KWh, battery is 24\nat time 7 agent transacts 8 KWh, battery is 31\nat time 8 agent transacts 1 KWh, battery is 38\nat time 9 agent transacts 1 KWh, battery is 39\nat time 10 agent transacts 1 KWh, battery is 40\nat time 11 agent transacts -2 KWh, battery is 41\nat time 12 agent transacts -2 KWh, battery is 39\nat time 13 agent transacts -2 KWh, battery is 37\nat time 14 agent transacts -2 KWh, battery is 35\nat time 15 agent transacts -2 KWh, battery is 33\nat time 16 agent transacts 2 KWh, battery is 31\nat time 17 agent transacts 1 KWh, battery is 33\nat time 18 agent transacts -2 KWh, battery is 34\nat time 19 agent transacts -2 KWh, battery is 32\nat time 20 agent transacts -1 KWh, battery is 30\nat time 21 agent transacts -2 KWh, battery is 29\nat time 22 agent transacts 2 KWh, battery is 27\nat time 23 agent transacts -2 KWh, battery is 29\nat time 24 agent transacts 2 KWh, battery is 27\nat time 1 agent transacts 2 KWh, battery is 29\nat time 2 agent transacts 2 KWh, battery is 31\nat time 3 agent transacts 2 KWh, battery is 33\nat time 4 agent transacts 8 KWh, battery is 35\nat time 5 agent transacts 8 KWh, battery is 42\nat time 6 agent transacts 1 KWh, battery is 49\nat time 7 agent transacts -3 KWh, battery is 50\nat time 8 agent transacts 1 KWh, battery is 47\nat time 18 agent transacts -1 KWh, battery is 27\nat time 19 agent transacts -23 KWh, battery is 26\nat time 20 agent transacts 0 KWh, battery is 0\nat time 21 agent transacts 0 KWh, battery is 0\nat time 22 agent transacts 0 KWh, battery is 0\nat time 23 agent transacts 0 KWh, battery is 0\nat time 24 agent transacts 0 KWh, battery is 0\nat time 1 agent transacts 1 KWh, battery is 0\nat time 2 agent transacts 1 KWh, battery is 1\nat time 3 agent transacts -1 KWh, battery is 2\nat time 4 agent transacts 8 KWh, battery is 1\nat time 5 agent transacts 8 KWh, battery is 8\nat time 6 agent transacts 8 KWh, battery is 15\nat time 7 agent transacts 8 KWh, battery is 22\nat time 8 agent transacts 1 KWh, battery is 29\nat time 18 agent transacts -1 KWh, battery is 9\nat time 19 agent transacts -1 KWh, battery is 8\nat time 20 agent transacts -1 KWh, battery is 7\nat time 21 agent transacts -1 KWh, battery is 6\nat time 22 agent transacts -1 KWh, battery is 5\nat time 23 agent transacts -1 KWh, battery is 4\nat time 24 agent transacts -1 KWh, battery is 3\nat time 1 agent transacts 1 KWh, battery is 2\nat time 2 agent transacts 1 KWh, battery is 3\nat time 3 agent transacts -1 KWh, battery is 4\nat time 4 agent transacts 1 KWh, battery is 3\nat time 5 agent transacts 8 KWh, battery is 4\nat time 6 agent transacts 8 KWh, battery is 11\nat time 7 agent transacts 8 KWh, battery is 18\nat time 8 agent transacts 2 KWh, battery is 25\nat time 9 agent transacts 2 KWh, battery is 27\nat time 10 agent transacts -2 KWh, battery is 29\nat time 11 agent transacts -1 KWh, battery is 27\nat time 12 agent transacts -1 KWh, battery is 26\nat time 13 agent transacts -1 KWh, battery is 25\nat time 14 agent transacts -1 KWh, battery is 24\nat time 15 agent transacts -1 KWh, battery is 23\nat time 16 agent transacts -1 KWh, battery is 22\nat time 17 agent transacts -1 KWh, battery is 21\nat time 18 agent transacts -1 KWh, battery is 20\nat time 19 agent transacts -1 KWh, battery is 19\nat time 20 agent transacts -1 KWh, battery is 18\nat time 21 agent transacts -1 KWh, battery is 17\nat time 22 agent transacts -1 KWh, battery is 16\nat time 23 agent transacts -1 KWh, battery is 15\nat time 24 agent transacts -1 KWh, battery is 14\nat time 1 agent transacts -1 KWh, battery is 13\nat time 2 agent transacts -1 KWh, battery is 12\nat time 3 agent transacts -1 KWh, battery is 11\nat time 4 agent transacts -1 KWh, battery is 10\nat time 5 agent transacts 8 KWh, battery is 9\nat time 6 agent transacts 8 KWh, battery is 16\nat time 7 agent transacts 1 KWh, battery is 23\nProfit on validation set:  -711.3911200000026\n","output_type":"stream"}]},{"cell_type":"code","source":"# class DDQNAgent:\n    \n#     def __init__(self, env, device, epsilon_decay, \n#                  epsilon_start, epsilon_end, discount_rate, lr, buffer_size, seed = 123):\n#         '''\n#         Params:\n#         env = environment that the agent needs to play\n#         device = set up to run CUDA operations\n#         epsilon_decay = Decay period until epsilon start -> epsilon end\n#         epsilon_start = starting value for the epsilon value\n#         epsilon_end = ending value for the epsilon value\n#         discount_rate = discount rate for future rewards\n#         lr = learning rate\n#         buffer_size = max number of transitions that the experience replay buffer can store\n#         seed = seed for random number generator for reproducibility\n#         '''\n#         self.env = env\n#         self.device = device\n#         self.epsilon_decay = epsilon_decay\n#         self.epsilon_start = epsilon_start\n#         self.epsilon_end = epsilon_end\n#         self.discount_rate = discount_rate\n#         self.learning_rate = lr\n#         self.buffer_size = buffer_size\n        \n#         self.replay_memory = ExperienceReplay(self.env, self.buffer_size)\n#         self.online_network = DQN(self.env, self.learning_rate).to(self.device)\n        \n#         self.target_network = DQN(self.env, self.learning_rate).to(self.device)\n#         self.target_network.load_state_dict(self.online_network.state_dict())\n    \n#     def choose_action(self, step, observation, greedy = False):\n        \n#         '''\n#         Params:\n#         step = the specific step number \n#         observation = observation input\n#         greedy = boolean that\n        \n#         Returns:\n#         action: action chosen (either random or greedy)\n#         epsilon: the epsilon value that was used \n#         '''\n        \n#         epsilon = np.interp(step, [0, self.epsilon_decay], [self.epsilon_start, self.epsilon_end])\n#         random_sample = random.random()\n#         mask = self.env.mask()\n        \n#         if (random_sample <= epsilon) and not greedy:\n#             #Random action\n#             action = np.random.choice(np.arange(self.env.action_space.n)[mask])\n#         else:\n#             #Greedy action\n#             obs_t = torch.as_tensor(observation, dtype = torch.float32, device=self.device)\n#             q_values = self.online_network(obs_t.unsqueeze(0))\n#             max_q_index = torch.as_tensor(torch.argmax(q_values.squeeze(0)[mask]).item() + np.where(mask)[0][0])\n#             action = max_q_index.detach().item()  \n#         return action, epsilon\n    \n#     def learn(self, batch_size):\n        \n#         '''\n#         Here we do gradient descent      \n#         Params:\n#         batch_size = number of transitions that will be sampled\n#         '''\n        \n        \n#         #Sample random transitions with size = batch size\n#         observations_t, actions_t, rewards_t, dones_t, new_observations_t, new_masks_t = self.replay_memory.sample(batch_size)\n#         target_q_values = self.online_network(new_observations_t)\n#         target_q_values[~new_masks_t] = -50\n#         max_target_q_values = target_q_values.max(dim=1, keepdim=True)[0] \n#         targets = rewards_t + self.discount_rate * (1-dones_t) * max_target_q_values\n        \n\n#         #Compute loss\n#         q_values = self.online_network(observations_t)\n        \n#         action_q_values = torch.gather(input=q_values, dim=1, index=actions_t)\n#         #Loss:  Huber loss\n#         loss = F.smooth_l1_loss(action_q_values, targets.detach())\n#         #Uncomment this line to use the standard MSE loss\n#         #loss = F.mse_loss(action_q_values, targets.detach())\n\n#         #Solution:\n#         #Gradient descent\n#         self.online_network.optimizer.zero_grad()\n#         loss.backward()\n#         self.online_network.optimizer.step()\n        \n#     def predict(self):\n#         '''\n#         Params:\n#         step = the number of the step within the epsilon decay that is used for the epsilon value of epsilon-greedy\n#         seed = seed for random number generator for reproducibility\n#         '''\n        \n#         action = self.choose_action(self.env.current_step, self.env.current_state, True)[0] \n        \n#     def update_target_network(self):\n#         self.target_network.load_state_dict(self.online_network.state_dict())\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2024-01-26T13:17:35.017779Z","iopub.execute_input":"2024-01-26T13:17:35.018364Z","iopub.status.idle":"2024-01-26T13:17:35.028119Z","shell.execute_reply.started":"2024-01-26T13:17:35.018327Z","shell.execute_reply":"2024-01-26T13:17:35.026754Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"# dagent = DDQNAgent(train_env, device, epsilon_decay, epsilon_start, epsilon_end, discount_rate, lr, buffer_size)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-26T13:17:35.031593Z","iopub.execute_input":"2024-01-26T13:17:35.032234Z","iopub.status.idle":"2024-01-26T13:17:35.044796Z","shell.execute_reply.started":"2024-01-26T13:17:35.032194Z","shell.execute_reply":"2024-01-26T13:17:35.043622Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"# average_rewards_ddqn = training_loop(train_env, dagent, max_episodes, target_ = True) ","metadata":{"execution":{"iopub.status.busy":"2024-01-26T13:17:35.046103Z","iopub.execute_input":"2024-01-26T13:17:35.047178Z","iopub.status.idle":"2024-01-26T13:17:35.058089Z","shell.execute_reply.started":"2024-01-26T13:17:35.047140Z","shell.execute_reply":"2024-01-26T13:17:35.056775Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"# eval_env = SmartGridEnv(val)\n\n# dagent.env = eval_env\n# i=0\n# while not dagent.env.done:\n#     current_state = tuple(dagent.env.current_state)\n#     action = dagent.predict()\n#     if i<100:\n#         print(f\"at time {dagent.env.current_hour +  1} agent transacts {dagent.env.to_discrete(action)} KWh, battery is {dagent.env.current_battery}\")\n#     next_state, reward, done, available = dagent.env.step(action)\n#     i=i+1\n\n# print(\"Profit on validation set: \", eval_env.profit)","metadata":{"execution":{"iopub.status.busy":"2024-01-26T13:17:35.059529Z","iopub.execute_input":"2024-01-26T13:17:35.060502Z","iopub.status.idle":"2024-01-26T13:17:35.069040Z","shell.execute_reply.started":"2024-01-26T13:17:35.060466Z","shell.execute_reply":"2024-01-26T13:17:35.067502Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}